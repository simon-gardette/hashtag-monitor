{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import os\n",
    "import json\n",
    "from IPython.display import JSON\n",
    "import logging\n",
    "import parser\n",
    "import traceback\n",
    "import sys\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oauth handling (probably should do a class later). Shouldalso use an generator for secret and key but as twitter doesn't expire it it will do for the moment. \n",
    "In a production version I would probably generate an app by account so I can track each account usage. \n",
    "\n",
    "Oauth 2 could also psossibly be a solution as I only need public infos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_key = os.environ['TWITTER_API_KEY']\n",
    "consumer_secret = os.environ['TWITTER_API_SECRET']\n",
    "key = os.environ['TWITTER_ACCESS_TOKEN']\n",
    "secret = os.environ['TWITTER_ACCESS_SECRET']\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(key, secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using tweepy \n",
    "The main objective is to gather infos from twitter. We're gonna use the stream endpoint because using the search doesn't return all the tweets.\n",
    "\n",
    "Probably end up bumping into api limitations pretty fast but it will be ok for the demo.\n",
    "\n",
    "The process will be the following : \n",
    "\n",
    "- Get latest tweet \n",
    "- Store Raw response (probably in an S3/minio bucket. will probably do it in second time but it should be a good idea in case the processing crashes\n",
    "- get relevant datas\n",
    "- run additional infos gathering and transform \n",
    "      - detect mood ? \n",
    "      - lang detection ? \n",
    "      - ...\n",
    "- store in the sql base (shared) \n",
    "\n",
    "Airflow process : \n",
    "- Each hour detect new keywords. \n",
    "- For each keyord create a notebook (aka a thread) \n",
    "- Check if all notebook Running. \n",
    "- can mark a keyword as inactive in Flask then  don't check \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up our main class using tweepy.StreamListener\n",
    "class TwitterListener(tweepy.StreamListener):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.database = os.environ['SHARED_DB_URI']\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        if status_code == 420:\n",
    "            # returning False in on_data disconnects the stream\n",
    "            return False\n",
    "\n",
    "    def on_status(self, status):\n",
    "        print(status.text)\n",
    "        return True\n",
    "\n",
    "    def on_data(self, data):\n",
    "        \"\"\"\n",
    "        Automatic detection of the kind of data collected from Twitter\n",
    "        This method reads in tweet data as JSON and extracts the data we want.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # parse as json\n",
    "            raw_data = json.loads(data)\n",
    "\n",
    "            # extract the relevant data\n",
    "            if \"text\" in raw_data:\n",
    "                user = raw_data[\"user\"][\"screen_name\"]\n",
    "                created_at = raw_data[\"created_at\"]\n",
    "                tweet = raw_data[\"text\"]\n",
    "                retweet_count = raw_data[\"retweet_count\"]\n",
    "                id_str = raw_data[\"id_str\"]\n",
    "\n",
    "            #insert data just collected into MySQL my_database\n",
    "            #populate_table(user, created_at, tweet, retweet_count, id_str)\n",
    "            print(f\"Tweet colleted at: {created_at}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(traceback.format_exc())\n",
    "            # Logs the error appropriately. \n",
    "            \n",
    "\n",
    "# todo : create the tables on init\n",
    "    def populate_table(\n",
    "        self, user, created_at, tweet, retweet_count, id_str, raw_data\n",
    "    ):\n",
    "        \"\"\"Populate a given table witht he Twitter collected data\n",
    "\n",
    "        Args:\n",
    "            user (str): username from the status\n",
    "            created_at (datetime): when the tweet was created\n",
    "            tweet (str): text\n",
    "            retweet_count (int): number of retweets\n",
    "            id_str (int): unique id for the tweet\n",
    "            raw_data (json) : storing raw data for further usage\n",
    "        \"\"\"\n",
    "\n",
    "        dbconnect = connect_db(self.database)\n",
    "\n",
    "        cursor = dbconnect.cursor()\n",
    "        cursor.execute(\"USE airflowdb\")\n",
    "\n",
    "        # add content here\n",
    "\n",
    "        try:\n",
    "            # what is missing? \n",
    "            commit()\n",
    "            print(\"commited\")\n",
    "\n",
    "        except mysql.Error as e:\n",
    "            print(e)\n",
    "            dbconnect.rollback()\n",
    "\n",
    "        cursor.close()\n",
    "        dbconnect.close()\n",
    "\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet colleted at: Thu Jan 09 23:52:13 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:13 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:19 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:19 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:20 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:23 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:23 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:38 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:38 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:38 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:41 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:43 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:46 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:52 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:53 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:54 +0000 2020\n",
      "Tweet colleted at: Thu Jan 09 23:52:57 +0000 2020\n"
     ]
    }
   ],
   "source": [
    "tweepy_listener = TwitterListener()\n",
    "tweepy_stream = tweepy.Stream(auth = api.auth, listener=tweepy_listener)\n",
    "results = tweepy_stream.filter(track=['Minecraft'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
